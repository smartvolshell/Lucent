首先介绍一下在replica set里分为三种节点类型：
1. primary  负责client的读写。
2. secondary作为热备节点，应用Primary的oplog读取的操作日志,和primary保持一致，不提供读写操作！
  secondary有两种类型: 
  1).normal secondary   随时和Primay保持同步,  
  2).delayed secondary  延时指定时间和primary保持同步,防止误操作. 
3. arbiter.它不负责任何读写,只作为一个仲裁者，负责primary down的时候剩余节点的选举操作.

 在Replica Set 如果primary down了，要进行故障切换，集群的选举策略：
	当primary down了之后，剩下的节点会选择一个primary节点，仲裁节点也会参与投票，
	避免僵局出现(如果没有仲裁节点，对于两节点的replica set 从节点down，主节点会变为secondary，导致整个replica set 不可用)
	选择依据为：
		优先级最高的且数据新鲜度最新的！

	 primary 节点使用心跳来跟踪集群中有多少节点对其可见。如果达不到1/2，活跃节点会自动降级为secondary。
	 这样就能够防止上面说的僵局状态或者当网络切割后primary已经与集群隔离的时候！

当选出新的primary之后，此数据库的数据就会被假定为整个集群中的最新数据，对其他节点(原来的活跃节点)的操作都会回滚，
	即便之前的primary已经恢复工作了。为了完成回滚，后都要重新所有节点连接新的主库进行同步。


配置方法

	两个节点的Replica Sets，启动后一个选举成为Primary，一个成为Secondary。
问题

	这时候如果Secondary宕机，那么Primary会怎么样呢？Primary会立刻变成Secondary！这时候集群里没有Primary了！
	为什么会出现这样的情况呢。
原因

	这是和MongoDB的Primary选举策略有关的，试想如果情况不是Secondary宕机，而是网络断开，那么两个节点都会选取自己为Primary，
	因为他们能连接上的只有自己这一个节点。而这样的情况在网络恢复后就需要处理复杂的一致性问题。而且断开的时间越长，
	时间越复杂。所以MongoDB选择的策略是如果集群中只有自己一个节点，那么不选取自己为Primary。
解决方法

	所以正确的做法应该是添加两个以上的节点，或者添加arbiter，当然最好也最方便的做法是添加arbiter，aribiter节点只参与选举，
	几乎不会有压力，所以你可以在各种闲置机器上启动arbiter节点，这不仅会避免上面说到的无法选举Primary的情况，
	更会让选取更快速的进行。（因为如果是三台数据节点，一个节点宕机，另外两个节点很可能会各自选举自己为Primary，
	从而导致很长时间才能得出选举结果）



Replica Set 选举过程
心跳检测

     假设我们有三个节点的replica sets：X，Y和Z节点。
     在replica sets结构中，这三个节点每2秒会各自向其它两个节点发送一个心跳检测请求。
     比如X节点向Y和Z节点各发送了一个心跳检测请求，在正常情况下，Y、Z会返回一个包含自身信息的回复包，
     回复包中主要包括了下面一些信息：它们现在是什么角色（primary 还是 secondary)，他们是否能够在必要的时候成为 primary，以及他们当前时钟时间等等。

    X节点在收到回复包后，会用这些信息更新自己的一个状态映射表，更新的内容包括：是否有新的节点加入或有老的节点宕掉，这个请求的网络传输时间等等。

    而当X节点的映射表发生了变化，那X会进行下面的逻辑判断：
	如果X是 primary，而另外一个节点出现故障，那么它会观察自己是否还能和集群中大多数节点进行通信，
		如果不能与大多数节点通信，那么他会把自己从 primary 降级为 secondary。
	（在replica sets中，primary 必须能够和集群中的大多数节点进行通信，以免发生网络断开形成两个或多个节点群各自为政的情况，这样会影响到数据的一致性）
关于降级

	在节点从 primary 降级为 secondary 的过程中，会有一些问题出现。在 MongoDB 中，写操作默认是通过 fire-and-forget 的模式来进行的，也就是说写操作通常不关心是否成功，发完请求后客户端就认为成功了。但如果这时候 primary 进行降级操作，那么客户端并不知道这时候 primary 已经降级成为 secondary 了，客户端可能还会将后续的写操作发送给这个节点。这时候刚刚降级的这个 secondary 可以发送一个包说“我已经不是 primary 了”，但是我们上面说过了，客户端根本就无视你这个包。所以客户端根本不知道这次写入已经失败了。

	对于这个问题，你可能会说”那我们每次都使用安全写入不就行了“（安全写入意思是说等待服务器返回成功后客户端才认为写成功了），但是很明显，这非常不靠谱。所以我们的做法是，在一个 primary 降级成为 secondary 后，它会将原来的所有连接关闭。
	这样客户端在下一次写入的时候就会出现 socket 错误。而客户端在发现这个错误之后，就会重新向集群获取新的 primary 的地址，并将后续的写操作都往新的服务器上写入。

选举

	我们回头再来看心跳监测请求：如果X是一个 secondary，那么X会定时检测是否需要选举自己成为 primary。
	其检测内容包括：
		是否集群中有其它节点认为自己是 primary？
		X节点自己是否已经是 primary？
		X节点自己是否有资格成为 primary？
	如果这三个问题中的任何一个回答是肯定的，那么X节点就不会试图把自己变成primary。
	也就是说，只有当X节点是一个能够当 primary 的secondary，并且其它节点都不是primary时，X才会发起选举并选自己为primary。

	而当X发现现在需要一个 primary 并且自己又正好可以充当时，它就会发起一轮选举：X节点会向Y、Z节点各发起一个请求包，告知他们”我认为我可以接管 primary 的角色，你们觉得怎么样？“

	当Y和Z收到上面的请求包时，他们会进行下面几项检测：
		他们是否已经知道集群中有一个 primary了？
		他们自己的数据是否比X节点更新？
		是否有其它节点的数据比X节点更新？
	如果上面条件有任何一个满足，那么他们都会认为X不够资格成为 primary，他们会发送一个返回包告知X说”停止选举！“。
	而如果三个条件都不成立，也就是说他们认为目前集群中确实没有 primary，并且X的数据又是最新的，那么他们会发送返回包告知X说”没问题“。

	如果X收到”停止选举！“的返回，那么他会马上停止选举并保持自己为 sencondary 状态。

	如果X收到所有其它节点都返回说”没问题“，那么他会进入选举过程的第二阶段。

	在第二阶段中，X会向其它节点发送一个包，说”我宣布我已经是 primary 了“。这时候，Y和Z节点再进行一些最终的确认：
	上面的判断过的所有条件是否依然表明X可以做 primary，
	如果确实如此，那么他们会在本轮 primary 选举中向X出赞成票。并且他们投完赞成票后，30秒内不会再做其它投票决定。

	上面是说如果第二次确认还是通过的情况，那么如果最终确认没有通过呢。他们会投一个反对票，反对X成为 primary，如果有反对票产生，那么这一轮选举就失败了。X还是保持 secondary 的身份。

	下面我们假设一种情况，如果Y给X投了赞成票，而Z给X投了反对票。那这时候Y由于投了赞成票，它在30秒内不能再进行投票。所以如果这时候Z发起选举想让自己成为 primary，那么Z这时候必须要获得X的赞成票。因为这时候Y不能投票，为了获取多数票，Z必须获得X的赞成票。

	所以投票的规则是这样的：如果没有人投反对票，并且赞成票的比例过半，那么本轮选举对象就能够成为 primary。

同步过程

	一个健康的secondary在运行时，会选择一个离自己最近的，数据比自己新的节点进行数据同步。
选定节点后，它会从这个节点拉取oplog同步日志，具体流程是这样的：

    执行这个op日志
    将这个op日志写入到自己的oplog中(local.oplog.rs)
    再请求下一个op日志

如果同步操作在第1步和第2步之间出现问题宕机，那么secondary再重新恢复后，会检查自己这边最新的oplog，由于第2步还没有执行，所以自己这边还没有这条写操作的日志。
这时候他会再把刚才执行过的那个操作执行一次。那对同一个写操作执行两次会不会有问题呢？
	MongoDB在设计oplog时就考虑到了这一点，所以所有的oplog都是可以重复执行的，比如你执行 {$inc:{counter:1}} 对counter字段加1，counter字段在加1 后值为2，那么在oplog里并不会记录 {$inc:{counter:1}} 这个操作，而是记录 {$set:{counter:2}}这个操作。
所以无论多少次执行同一个写操作，都不会出现问题。
w参数

当我们在MongoDB时执行一个写操作时，默认会直接返回成功，同时也可以通过设置w参数，指定这个写操作同步到几个节点后才返回成功。如下：

	db.foo.runCommand({getLastError:1, w:2})

上面例子就是执行getLastError命令，使其在上一个写操作同步到两个节点上后再返回。不同的客户端可能在写法上不太一样，不过这个功能应该都是有的。
对于重要数据，可以考虑采用这样的方式，通过牺牲一部分写性能来提升数据的安全性。

这个功能是如何实现的呢，primary节点是如何知道数据同步了几份呢？

在调用上面命令时，实际上MongoDB内部执行了如下的一些流程：

    1.在primary上完成写操作
    2.在primary上记录一条oplog日志，日志中包含一个ts字段，值为写操作执行的时间，比如本例中记为t
    3.客户端调用{getLastError:1, w:2}命令等待primary返回结果
    4.secondary从primary拉取oplog，获取到刚才那一次写操作的日志
    5.secondary按获取到的日志执行相应的写操作
    6.执行完成后，secondary再获取新的日志，其向primary上拉取oplog的条件为{ts:{$gt:t}}
    7.primary此时收到secondary的请求，了解到secondary在请求时间大于t的写操作日志，所以他知道操作在t之前的日志都已经成功执行了
    8.这时候getLastError命令检测到primary与secondary都完成了这次写操作，于是 w:2 的条件满足了，返回给客户端成功

链式同步结构

上面对w参数的实现，讲解上比较简单，只讲了w为2的情况，但是当w更大时，由于我们并不是采用一主多从的方式进行同步。所以情况会复杂一些。

比如我们有节点A，为primary节点，然后B节点为secondary节点，它从A节点同步数据，同时又有secondary节点C，它从同是secondary的B节点同步数据。这样A->B->C之间就形成了一个链式的同步结构。如果我们设定w为3，那么A节点如何能知道C节点已经从B节点同步成功了呢？
	这是通过oplog同步协议来实现的，我们用通俗的语言来解释一下oplog的同步协议。

    1.当C从B同步数据时，C会在协议中对B说：我要从你这同步数据了，如果写操作有w参数的话，我的同步也算上吧。
    2.然后B会回答说：我不是一个primary节点，我会把你的这个计数转到我的同步源上去。
    3.然后B再对A打开一个新的连接，并且对A说：这个连接你就当成是C的吧，也算一个计数在w里。
    4.这时候在A看来，就有两个连接连到他上面，一个是B，一个是虚拟的C，这两个连接都能报告他说完成了同步操作。

当一个写操作在A上执行后，B首先同步到这个操作的oplog，执行完后会告诉A，我执行完了。
然后C同样从B上获取到B的oplog，也执行了这一条写操作，然后他告诉B，我执行完了，
B在收到这个响应后，会通过刚才开通的虚拟通道跟A说，我是虚拟的C节点，我也完成写操作了。
这时候A就知道，A、B、C三个节点都完成写操作了。w：3的条件满足，然后返回给调用getLastError的客户端，完成这次操作。



